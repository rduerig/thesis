
\section{Concepts}
Here are general game playing concepts and definitions:

\begin{description}
\item[Perfect Information] A game has the property of perfect information when both players know the full state of the game. There is no hidden information, such as cards in a hand, nor elements of randomness such as a dice.
\item[Zero Sum Games] A zero sum game has the property that one players gain is the other players loss. A draw is still possible, but no move can help both players, so there is no incentive to cooperate.
\item[State] A state is a full description of a board position. It includes the locations of all the pieces, and any other relevant information. In chess, the state would include whether each king can castle, and whether a pawn can capture en passant. In games where repeated moves are not allowed, the full game history may be included in the state. Occasionally a simplified version of the state is used when speed is more important than accuracy.
\item[Move] A move is a distinct action by one of the players. In games with multi-part moves, such as Amazons where each move consists of a movement plus shooting an arrow, the pair of actions would be considered a single move. There are usually multiple moves available from each state, but only one can be chosen per turn.
\item[Branching Factor] is the number of moves available to each player on average. This depends on the rules of the game, board size, pieces in play and the stage of the game. This can be as low as 1 for forced moves, or very high, such as in the hundreds or thousands for Amazons or Arimaa.
\item[Game Tree] Games can be represented as a game tree. Each position in the game is a node, and each move is an edge in the graph connecting the position before the move to the position after the move. When there are multiple paths to a position, it can be represented as separate nodes, leading to a tree, or combined as a single node, leading to a directed acyclic graph (DAG). Some games have loops, where a position can be reached multiple times in a single game, leading to a directed graph.
\item[Node Value] Each node has an associated outcome or expected outcome associated with it. Terminal positions are positions where one of the players has won or it is a draw, have an exact value such as win, loss or draw, or a score to show how much a player won by.
\item[Heuristic] A heuristic function takes a position and returns a heuristic value associated with the position. This value often representing a likelihood of winning from that position, but can also be just an abstract number that can be compared against other heuristic values to order nodes or moves.
\item[State Complexity] is the number of unique reachable states in the game.
\item[Minimax] In 2-player games, each player attempts to win at the expense of the other player. To do so, each player attempts to minimize the opponents gain while maximizing his own gain.
\item[Minimax Backup] Given a node N who's children all have known values, N's value is equal to the value of the most favourable child for the current player.
\item[Minimax Value] The value of a node given both players play perfectly according to Minimax.
\end{description}

Game playing programs all build a game tree, and then chose the most promising move. 



\section{Alpha-Beta}

Alpha-beta ($\alpha\beta$) is a refinement of minimax, ignoring or pruning parts of the game tree that are provably unreachable if both players play perfectly. It maintains two bounds to store the minimum value each player is guaranteed given the tree searched so far. When these bounds cross, the remaining moves don't need to be considered. Alpha-beta is a depth first algorithm, but has many variants that improve move ordering or make it act more like a best-first algorithm.

Requires a heuristic, depends on the speed and strength of the heuristic.

\subsection{Transposition Table}

\subsection{Negamax}

\subsection{Other extensions}

Negascout, history heuristic, killer move, quiescence search, etc



\section{Proof Number Search}




\section{Monte Carlo Tree Search}

Monte Carlo Tree Search is an algorithm for building and exploring a game tree. It consists of four phases:
\begin{description}
\item[Descent] This phase descends the game tree from the root to a leaf node in the game tree. At each node one of the available moves is selected according to some criteria based on previous performance or heuristic knowledge. When the Upper Confidence Bounds (UCB) formula is used, this is called Upper Confidence Bounds applied to Trees (UCT), but other formulas have been developed and are commonly used. RAVE is a modification of this formula.
\item[Expansion] If the leaf node has has experience from a previous simulation, its children are expanded, increasing the size of the tree.
\item[Rollout] A random game is played from the leaf node through the newly expanded children, to the end of the game. Heuristics can be used to make the moves less random and more representative of a real game. The strength of the overall algorithm is highly dependent on the average outcome of the random games being representative of the strength of the position.
\item[Back-propagation] The outcome of the random game in the rollout is back propagated to the moves chosen in the tree. The winning rate of the moves made by the player that won the rollout is increased while winning rate of the moves by the player that lost the rollout is decreased.
\end{description}

These four steps are repeated continually until a stopping condition such as running out of time or memory. At this point a move is chosen by some criteria. The three most common criteria are: most experience, most wins and highest lower confidence bound.

Many extensions have been developed to increase the playing strength of MCTS

\begin{itemize}
\item RAVE
	\begin{itemize}
		\item $\beta*v_i + (1-\beta)*r_i$
		\item $\beta = k/(k+n_i)$
		\item $\beta = \sqrt{k/(3n_i+k)}$
		\item $\beta = r_i/(n_i+r_i+4*n_i*r_i*b^2)$
	\end{itemize}
\item Heuristic Knowledge
	\begin{itemize}
		\item Game specific state knowledge
		\item Experience initialization
		\item Rave initialization
		\item Extra term: $ + k/\sqrt{n_i}$
	\end{itemize}
\item Non-random rollouts
	\begin{itemize}
		\item Game specific knowledge/patterns
		\item Weighted random, by rave or heuristic knowledge
		\item Last good reply
		\item forced moves/instant wins
	\end{itemize}
\item parallelization
\item dynamic widening
\item Solution backups
\item multiple rollouts per simulation
\item avoid symmetries or transpositions
\item first player urgency

\end{itemize}















